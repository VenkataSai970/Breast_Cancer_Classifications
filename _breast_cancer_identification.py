# -*- coding: utf-8 -*-
"""_Breast_Cancer_Identification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jlI84nBigNcxc1PRhSG5BAsYTqgehbzb

<a href="https://colab.research.google.com/github/edaaydinea/machine-learning/blob/master/Breast%20Cancer%20Classification/%20Breast_Cancer_Classification.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## **BREAST CANCER CLASSIFICATION**
### **Via SuperDataScience Team**

*   Breast cancer is the most common cancer among women worldwide accounting for 25% of all cancer cases and infected two point one million people in 2015.
*   Ealy diagnosis significantly increase the chances of survival.
*   The key challenge in cancer detection is how to classify tumours into *malignant* or *benign* machine learning techniques can dramatically improve the accuracy of diagnosis.
*   Research indicates that most experienced physicians can diagnose cancer with 79% accuracy.


**First stage**: Any process which is simply extracting some of the cells out of the tumour

When we say benign that means the tumour is kind of not spreading across the body so the patient is safe somehow if it's malignant that means it's a cancerous.
That means we need to intervene and actually stopping cancer growth.

---



What we do here in the machine learning aspect.
* We excute all these images and
* We wanted to specify if that cancer out of these image is malignant or benign.

So what we do with that we extract out of these images some features when we see features that mean some characteristics out of the image such as
* radius
* cells
* texture
* perimeter
* area
* smoothness

We feed all these features in to kind of our machine learning model.

**MAIN PART:**  We want to teach the machine how to basically classify images or classify data and tell us if it's malignant or benign.

## **PROBLEM IN MACHINE LEARNING VOCABULARY**

*Input:* 30 feautures
* Radius
* Texture
* Perimeter
* Area
* Smoothness
* ...

*Target Class:* 2
* Malignant
* Benign

*How many datasets we have? :*
* Number of Instances : 569
* Class Distribution: 212 Malignant, 357 Benign

*Data source:*
* [Breast Cancer Wisconsin(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)
* [Breast Cancer Detection with Reduced Feature Set](https://www.researchgate.net/publication/271907638_Breast_Cancer_Detection_with_Reduced_Feature_Set)


We're going to say look if you look at these features then indicate that the cancer is let's say zero which is malignant in this case.

And then if we look at the 30 features may be classified as one which kind of a benign.

So  it's kind of the opiate is binary in a forum indicating zero or one for malignant or benign.

---
**SUPPORT VECTOR MACHINE CLASSIFIER**

Near the max Margin Hyperplane, we don't know whether this cancer is malignant or benign.

That's why the support vector machine classifier is very unique in this sense. It's simply uses the points or the support vectors that are on the boundary to draw the boundary out to classify the classes.

Support vector machines are really powerful techniques.
Why? Because it's kind of an extreme algorithm.
It just focus on the support of the suppor vectors or the points on the boundary and seperate them somehow.

**IMPORTING DATA**
"""

# import libraries
import pandas as pd # Import Pandas for data manipulation using dataframes
import numpy as np # Import Numpy for data statistical analysis
import matplotlib.pyplot as plt # Import matplotlib for data visualisation
import seaborn as sns # Statistical data visualization
# %matplotlib inline

# Import Cancer data drom the Sklearn library
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

cancer

# What dictionaries we have
cancer.keys()

# print them one by one
print(cancer['DESCR'])

print(cancer['target'])

print(cancer['target_names'])

print(cancer['feature_names'])

print(cancer['data'])

cancer['data'].shape

df_cancer = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))

df_cancer.head(5)

df_cancer.tail(5)

"""**VISUALIZING THE DATA**"""

sns.pairplot(df_cancer,vars= ['mean radius','mean texture', 'mean area', 'mean perimeter', 'mean smoothness'])

"""But the only problem is that doesn't show the target class. It doesn't show actual which one of these samples is malignant or which one of them is benign."""

sns.pairplot(df_cancer,hue = 'target', vars= ['mean radius','mean texture', 'mean area', 'mean perimeter', 'mean smoothness'])

"""The blue points in here that's the malignant case. The orange points in here that's the benign case."""

sns.countplot(df_cancer['target'])

"""We take one of these slide graphs and see how can we play."""

sns.scatterplot(x='mean area', y='mean smoothness', hue='target', data=df_cancer)

"""Let's check the correlation between the variables"""

plt.figure(figsize=(20,10))
sns.heatmap(df_cancer.corr(), annot=True)

"""**MODEL TRAINING (FINDING A PROBLEM SOLUTION)**"""

# Let's drop the target label coloumns
x = df_cancer.drop(['target'],axis=1)

x

y = df_cancer['target']
y

"""If you get a call that we have to take in our model we're going to do that we use a subset of our data for training and then after them on the list trained what we're going to do in order to test the model we're going to use the testing dataset which is data said that the modern has seen ever before."""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state= 5)

x_train

x_train.shape

x_test

x_test.shape

y_train

y_train.shape

y_test

y_test.shape

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

svc_model = SVC()

svc_model.fit(x_train, y_train)

"""**EVALUATING THE MODEL**

We're talking about the testing data which has data that has never seen before.
"""

y_predict = svc_model.predict(x_test)

y_predict

"""We're going to plot a confusion matrix.  We need to specify compare our true value versus the predicted that."""

cm = confusion_matrix(y_test, y_predict)

sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict))

"""**IMPROVING THE MODEL**"""

min_train = x_train.min()
min_train

range_train = (x_train - min_train).max()
range_train

x_train_scaled = (x_train - min_train)/range_train
x_train_scaled

sns.scatterplot(x = x_train['mean area'], y= x_train['mean smoothness'], hue= y_train)

sns.scatterplot(x= x_train_scaled['mean area'], y= x_train_scaled['mean smoothness'], hue= y_train)

min_test = x_test.min()
range_test = (x_test - min_test).max()
x_test_scaled = (x_test - min_test)/ range_test

svc_model.fit(x_train_scaled, y_train)

y_predict = svc_model.predict(x_test_scaled)

cm = confusion_matrix(y_test, y_predict)

sns.heatmap(cm, annot=True, fmt = 'd')

print(classification_report(y_test, y_predict))

"""**IMPROVING THE MODEL - PART 2**"""

param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']}

from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(SVC(), param_grid, refit= True, verbose= 4)

grid.fit(x_train_scaled, y_train)

grid.best_params_

grid.best_estimator_

grid_prediction = grid.predict(x_test_scaled)

cm = confusion_matrix(y_test, grid_prediction)

sns.heatmap(cm, annot=True)

print(classification_report(y_test,grid_prediction ))

"""**CONCLUSION**
* Machine Learning techniques (SVM) was able to classify tumors into Malignant / Benign with 97% accuracy.
* The technique can rapidly evaluate breast masses and classify them in an automated fashion.
* Early breast cancer can dramatically save lives especially in the developing world
* The technique can be further improved by combining Computer Vision/ ML techniques to directly classify cancer using tissue images.
"""